{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n",
    "    e.g.\n",
    "    Input: 'It is a nice day. I am happy.'\n",
    "    Output: ['it', 'is', 'a', 'nice', 'day', 'i', 'am', 'happy']\n",
    "    '''\n",
    "    tokens = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(data, seq_length, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of words, type: list\n",
    "    :param seq_length: the length of sequences,, type: int\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    return a dense sequence matrix whose elements are indices of words,\n",
    "    '''\n",
    "    data_matrix = np.zeros((len(data), seq_length), dtype=int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for j, word in enumerate(doc):\n",
    "            # YOUR CODE HERE\n",
    "            if j == seq_length:\n",
    "                break\n",
    "            word_idx = vocab_dict.get(word, 1) # 1 means the unknown word\n",
    "            data_matrix[i, j] = word_idx\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name, input_length, vocab=None, nrows=None):\n",
    "    \"\"\"\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name, nrows=nrows)\n",
    "    df['words'] = df['text'].apply(tokenize)\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        for i in range(len(df)):\n",
    "            for word in df.iloc[i]['words']:\n",
    "                vocab.add(word)\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict['<pad>'] = 0 # 0 means the padding signal\n",
    "    vocab_dict['<unk>'] = 1 # 1 means the unknown word\n",
    "    vocab_size = 2\n",
    "    for v in vocab:\n",
    "        vocab_dict[v] = vocab_size\n",
    "        vocab_size += 1\n",
    "\n",
    "    data_matrix = get_sequence(df['words'], input_length, vocab_dict)\n",
    "    stars = df['stars'].apply(int) - 1\n",
    "    return df['review_id'], stars, data_matrix, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_length, train_nrows=None, valid_nrows=None, test_nrows=None):\n",
    "    # Load training data and vocab\n",
    "    train_id_list, train_data_label, train_data_matrix, vocab = read_data(\"data/train.csv\", input_length, nrows=train_nrows)\n",
    "    K = max(train_data_label)+1  # labels begin with 0\n",
    "\n",
    "    # Load valid data\n",
    "    valid_id_list, valid_data_label, valid_data_matrix, vocab = read_data(\"data/valid.csv\", input_length, vocab=vocab, nrows=valid_nrows)\n",
    "\n",
    "    # Load testing data\n",
    "    test_id_list, _, test_data_matrix, _ = read_data(\"data/test.csv\", input_length, vocab=vocab, nrows=test_nrows)\n",
    "    \n",
    "    print(\"Vocabulary Size:\", len(vocab))\n",
    "    print(\"Training Set Size:\", len(train_id_list))\n",
    "    print(\"Validation Set Size:\", len(valid_id_list))\n",
    "    print(\"Test Set Size:\", len(test_id_list))\n",
    "    print(\"Training Set Shape:\", train_data_matrix.shape)\n",
    "    print(\"Validation Set Shape:\", valid_data_matrix.shape)\n",
    "    print(\"Testing Set Shape:\", test_data_matrix.shape)\n",
    "\n",
    "    # Converts a class vector to binary class matrix.\n",
    "    # https://keras.io/utils/#to_categorical\n",
    "    train_data_label = keras.utils.to_categorical(train_data_label, num_classes=K)\n",
    "    valid_data_label = keras.utils.to_categorical(valid_data_label, num_classes=K)\n",
    "    return train_id_list, train_data_matrix, train_data_label, \\\n",
    "        valid_id_list, valid_data_matrix, valid_data_label, \\\n",
    "        test_id_list, test_data_matrix, None, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 300\n",
    "embedding_size = 100\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.1\n",
    "total_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nrows = None\n",
    "valid_nrows = None\n",
    "test_nrows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_list, train_data_matrix, train_data_label, \\\n",
    "valid_id_list, valid_data_matrix, valid_data_label, \\\n",
    "test_id_list, test_data_matrix, _, vocab = load_data(input_length, train_nrows=train_nrows, valid_nrows=valid_nrows, test_nrows=test_nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train_data_matrix.shape[0]\n",
    "K = train_data_label.shape[1]\n",
    "\n",
    "input_size = len(vocab) + 2\n",
    "output_size = K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=input_size, output_dim=embedding_size, input_length=input_length))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(units=hidden_size))\n",
    "model.add(Dense(K, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'.format(train_score[0], train_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_score = model.evaluate(valid_data_matrix, valid_data_label, batch_size=batch_size)\n",
    "print('Validation Loss: {}\\n Validation Accuracy: {}\\n'.format(valid_score[0], valid_score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Save the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre = model.predict(test_data_matrix, batch_size=batch_size).argmax(axis=-1) + 1\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"review_id\"] = test_id_list\n",
    "sub_df[\"pre\"] = test_pre\n",
    "sub_df.to_csv(\"pre.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
