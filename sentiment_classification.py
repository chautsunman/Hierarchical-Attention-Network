# -*- coding: utf-8 -*-
"""attention_hierarchical.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KTW68Rp_MQ27XtFDwcwLl9uKzPfIxLQf

# Sentiment Classification

## Import Dependencies
"""

import re

import numpy as np
import pandas as pd

import tensorflow as tf

from keras.utils import to_categorical

import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords

from nltk.tokenize import sent_tokenize
nltk.download("punkt")
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from keras.layers import Input, Dense, Embedding, GRU, Bidirectional, Layer, TimeDistributed
from keras.models import Model
from keras import backend as K
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

from keras.utils import plot_model

"""## Trained Embeddings"""

# !wget http://nlp.stanford.edu/data/glove.6B.zip

# !unzip glove.6B.zip

embeddings_index = {}
with open("glove.6B.300d.txt", "r") as embeddings_file:
    for embedding_line in embeddings_file:
        embedding = embedding_line.split(" ")
        word = embedding[0]
        embeddings_index[word] = np.asarray(embedding[1:], dtype="float32")

"""## Main Function

## Hyperparameters
"""

max_words = 20000
max_words_per_sentence = 20
max_sentences_per_doc = 20
embedding_size = 300
batch_size = 128
learning_rate = 0.001
total_epoch = 5

"""## Preprocess data"""

stop_words = set(stopwords.words("english"))

def parse_text(data, max_words, max_words_per_sentence, max_sentences_per_doc, tokenizer=None):
    words = data

    words = words.apply(lambda doc: doc.lower().strip())
    words = words.apply(lambda doc: re.sub(r"([?.!,¿])", r" \1 ", doc))
    words = words.apply(lambda doc: re.sub(r'[" "]+', " ", doc))
    words = words.apply(lambda doc: re.sub(r"[^a-zA-Z?.!,¿]+", " ", doc))
    words = words.apply(lambda doc: doc.rstrip().strip())

    words = words.apply(lambda doc: doc.split(" "))
    words = words.apply(lambda doc: [word for word in doc if word not in stop_words])
    words = words.apply(lambda doc: " ".join(doc))

    if tokenizer is None:
        tokenizer = Tokenizer(num_words=max_words, oov_token="<UNK>")
        tokenizer.fit_on_texts(words)

    words = words.apply(lambda doc: sent_tokenize(doc))
    words = pad_sequences(
        words,
        maxlen=max_sentences_per_doc,
        dtype=object,
        padding="post",
        truncating="post",
        value=""
    )

    words = np.apply_along_axis(tokenizer.texts_to_sequences, 0, words)
    words = np.apply_along_axis(lambda sentences: pad_sequences(
        sentences,
        maxlen=max_words_per_sentence,
        padding="post",
        truncating="post"
    ), 1, words)

    return words, tokenizer

"""## Load the Data"""

train_data = pd.read_csv("data/train.csv")
valid_data = pd.read_csv("data/valid.csv")
test_data = pd.read_csv("data/test.csv")

train_stars = train_data["stars"].apply(int) - 1
train_label = to_categorical(train_stars, num_classes=5)
valid_stars = valid_data["stars"].apply(int) - 1
valid_label = to_categorical(valid_stars, num_classes=5)

train_words, tokenizer = parse_text(train_data["text"], max_words, max_words_per_sentence, max_sentences_per_doc)
valid_words, _ = parse_text(valid_data["text"], max_words, max_words_per_sentence, max_sentences_per_doc, tokenizer=tokenizer)
test_words, _ = parse_text(test_data["text"], max_words, max_words_per_sentence, max_sentences_per_doc, tokenizer=tokenizer)

embedding_weights = np.zeros((len(tokenizer.word_index) + 1, embedding_size))
for word, word_id in tokenizer.word_index.items():
    embedding = embeddings_index.get(word)
    if embedding is not None:
        embedding_weights[word_id] = embedding

"""## Attention Layer"""

class Attention(Layer):
    def __init__(self):
        super(Attention, self).__init__()

    def build(self, input_shape):
        self.context_vector = self.add_weight(
            name="context_vector",
            shape=(input_shape[1][2], 1),
            initializer="uniform",
            trainable=True
        )
        super(Attention, self).build(input_shape)

    def call(self, inputs):
        gru, hidden_hidden = inputs

        attention = K.dot(hidden_hidden, self.context_vector)

        attention = K.softmax(attention, axis=1)

        return K.sum(attention * gru, axis=1)

    def compute_output_shape(self, input_shape):
        return (input_shape[0][0], input_shape[0][2])

"""## Model"""

inputs = Input(shape=(max_sentences_per_doc, max_words_per_sentence))

words_input = Input(shape=(max_words_per_sentence, ))

embeddings = Embedding(
    len(tokenizer.word_index) + 1,
    embedding_size,
    input_length=max_words_per_sentence,
    weights=[embedding_weights],
    trainable=False
)(words_input)

gru = Bidirectional(GRU(
    50,
    activation="tanh",
    recurrent_activation="sigmoid",
    return_sequences=True
))(embeddings)

hidden_hidden = Dense(100, activation="tanh")(gru)

attention_layer = Attention()

attention_output = attention_layer([gru, hidden_hidden])

words_attention_model = Model(inputs=words_input, outputs=attention_output)

sentence_embeddings = TimeDistributed(words_attention_model)(inputs)

sentence_gru = Bidirectional(GRU(
    50,
    activation="tanh",
    recurrent_activation="sigmoid",
    return_sequences=True
))(sentence_embeddings)

sentence_hidden_hidden = Dense(100, activation="tanh")(sentence_gru)

sentence_attention_layer = Attention()

sentence_attention_output = sentence_attention_layer([sentence_gru, sentence_hidden_hidden])

sentiment = Dense(5, activation="softmax")(sentence_attention_output)

model = Model(inputs=inputs, outputs=sentiment)

words_attention_model.summary()

plot_model(model, "words_attention_model.png", show_shapes=True)

model.summary()

plot_model(model, "model.png", show_shapes=True)

optimizer = Adam(lr=learning_rate)

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

"""## Train the Model"""

model.fit(
    train_words,
    train_label,
    epochs=total_epoch,
    batch_size=batch_size,
    callbacks=[EarlyStopping(monitor="val_loss", patience=1, restore_best_weights=True)],
    validation_data=(valid_words, valid_label)
)

"""## Save the Model"""

filepath = "model.h5"

model.save(filepath)

"""## Test the model"""

train_score = model.evaluate(train_words, train_label, batch_size=batch_size)
print('Training Loss: {}\n Training Accuracy: {}\n'.format(train_score[0], train_score[1]))

valid_score = model.evaluate(valid_words, valid_label, batch_size=batch_size)
print('Validation Loss: {}\n Validation Accuracy: {}\n'.format(valid_score[0], valid_score[1]))

"""## Predict and Save the Result"""

test_pre = model.predict(test_words, batch_size=batch_size).argmax(axis=-1) + 1
sub_df = pd.DataFrame()
sub_df["review_id"] = test_data["review_id"]
sub_df["pre"] = test_pre
sub_df.to_csv("pre.csv", index=False)

